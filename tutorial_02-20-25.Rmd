---
title: "schell_lab_R_spatial_tutorial"
output: html_document
date: "2025-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Schell Lab R Spatial Tutorial
## Exploring remote sensing, census based and climatic datasets for urban ecology
### by Diego Ellis Soto and Yasmine Hentati
### February 25, 2025

This markdown assumes basic knowledge about GIS (e.g. rasters vs shapefiles, points vs lines, raster vs raster stack) and spatial ecology (e.g. issues related to grain size, spatial extent, spatio-temporal scale).

The motivation of this markdown is to provide a basic primer for downloading, annotating, visualizing and summarizing mostly static spatial data and relate it to UWIN camera trap locations in the Bay Area, California.

We'll work on both reading in and saving shapefiles/rasters in your study area, as well as extracting buffers from different layers around study area points (i.e. camera traps). 


## Packages
All of these are quite straightforward except for tidycensus, for which you'll need an API key. Use the directions under 2.1 [here](https://walker-data.com/census-r/an-introduction-to-tidycensus.html) to get it (just the short section before 2.1.1). Use the skeleton code below.

```{r}
install.packages("pacman") # this package just helps us quickly install and load a lot of packages at once 
pacman::p_load(tidycensus, sf, terra, mapview, tidyr, dplyr, readr, here, ggplot2, viridis, raster,units,sp,stars,exactextractr)

# see above link to get your tidycensus key and paste it below 
census_api_key("YOUR KEY HERE",
               install = TRUE)

Sys.getenv("CENSUS_API_KEY") # this should print your API key back to you 
```

## Reading in points 

Let's start by loading our UWIN trap camera and visualizing it.

First we'll read in our .csv and turn it into a spatial format R can work with (sf). Here we'll work with the UWIN/Schell Lab East Bay camera trap points (saved as a .csv directly from the first tab of the Master UWIN Bay Area spreadsheet). I like to use WGS84 / UTM projection (easting/northing) instead of ones that use lat/long, so we'll also convert the points so we can project it in UTMs. This will be useful to know if someone provides you with data in a certain projection and you want to change it to another projection.

```{r}

# read in data 
all_sites_df <- read_csv(here("data", "eastbay_sites.csv")) %>% 
  slice (1:72) %>% # keep only rows with data
  dplyr::select(1:17) %>% # keep only columns we might need 
  rename("Name" = "Site Names") 


```

Let's convert this dataframe to a spatial SF object and plot it using mapview.
```{r}
# convert the data frame into an sf format using the points columns 
all_sites <- st_as_sf(all_sites_df, coords=c("Long", "Lat"), crs="EPSG:4326") #the EPSG string is simply a project code -- 4326 is the code for WGS84 lat/long, which google maps and other systems use 

# use mapview to look at our sites and make sure everything seems to be in the right place 
mapview(all_sites) 

# transform all_sites to UTM with st_transform 
all_sites <- all_sites %>% st_transform("EPSG:32611") # EPSG code for WGS84 UTM zone 11 (california)

# take a look at it again
mapview(all_sites)
st_crs(all_sites) # check out our projection system 
```

### Read in median income data using tidycensus 

Instead of working with a shapefile we downloaded from the Internet, we'll use a nifty R package to download the data for us. Tidycensus uses data from American Community Survey with many different years of data available. 

The US census has hundreds of interesting variables, but some other variables we could explore in the future are income, road networks, racial demographics.

The link in the first packages section above is a great overall tutorial if you want to learn more.

```{r}

# load in ACS 2019 variables 
v19 <- load_variables(2019, "acs5", cache = TRUE)

# now load in sf  of california census tracts 
# with median household income as variable of interest
tractincomeCA <- get_acs(state = "CA", 
                         geography = "tract", 
                         variables = c(medincome="B19013_001"), # this is just the code for the med income data, i think i found it in the linked tutorial
                         geometry = TRUE) %>% 
  st_transform(crs = "EPSG:32611") # correct projection

# take a look at the data set
glimpse(tractincomeCA)
class(tractincomeCA)

# now let's only keep census tracts we want so the file isn't huge -- let's do the entire SF bay area in case we need the rest of this data one day
tractsSF <- tractincomeCA %>% dplyr::filter(substr(GEOID, 1, 5) 
                                            # these are GEOIDs which represent each county ("06" represents the state of CA) -- these can be found online 
                                            %in% c("06055", "06041", # napa, marin
                                                   "06095", "06013", # solano, contra costa 
                                                   "06001", "06075", # alameda, SF 
                                                   "06081", "06085")) # san mateo, san clara 

# view as a map -- mapview isn't for pub quality maps, but great for visualizing and interacting with your data 
mapview(tractsSF)

# let's make it look a bit nicer so we can get an idea of what the data actually looks like
mapview(tractsSF, zcol = "estimate", col.regions = viridis(100))

# let's change the colors and add our points on top 
map1 <- mapview(tractsSF, zcol = "estimate", col.regions = magma(100))
map2 <- mapview(all_sites, color = "blue", cex = 3) 

map1 + map2 



# now let's write this sf to a shapefile saved in our directory 
st_write(tractsSF, here("data", "sf_bay_med_income.shp"),
         append = FALSE)


```

### Read in housing density data 
This is an example of how to work with a shapefile you downloaded from the Internet. This data is publicly available from the Silvis lab. 
```{r}

bay_housing <- st_read(here("data", "CA_wui_block_1990_2020_change_v4.shp")) %>% 
  st_transform(crs = "EPSG:32611")# let's put it in the same projection we're working in 


# filter to only bay area counties
bay_housing <- ca_housing %>% dplyr::filter(substr(BLK20, 1, 5) # this is housing density by block for 2020 
                                           %in% c("06055", "06041", # same counties as before
                                                  "06095", "06013",
                                                  "06001", "06075",
                                                  "06081", "06085")) %>% 
  dplyr::select(BLK20, WATER20, POP2020,
                POPDEN2020, HUDEN2020,
                Shape_Leng:geometry) # keep only attributes we need 

# let's make all polygons with water (WATER20) NA so they don't get counted in the calculation (otherwise will show up at pop den 0)
bay_housing <- bay_housing %>%
  mutate(POPDEN2020 = ifelse(WATER20 == 1, NA, POPDEN2020))

st_write(bay_housing, here("data", , "bay_urban_huden_2020.shp"),
    append = FALSE)

```


### Rasters 
Awesome, we've learned how to read in and work with shapefiles! Now we can load some remote sensing variables in the form of rasters. These often come in the form of .tif or .img files. We can load these with the terra package. Older resources you find online might use the raster package, but it has since been deprecated. 

Common issues when working with rasters are: pixel size, projection, and extent. We also have to worry about projection and extent for shapefiles. It's especially important to keep track of these things when working in R, because you don't get as many opportunities to see what you are doing as in programs like ArcGIS. We will NOT cover how to mask water in this tutorial (e.g. apply an ocean mask), but you may also need to do this in study areas such as in the Bay.

Briefly: if you want to combine multiple rasters together in one ‘stack’ for analysis, these typically need to be at the same spatial extent and resolution. Since we are only extracting values of these rasters separately, we just need to ensure that these are in the same projection of our points (WGS84 in this example). There are literally hundreds of projections and many more are more suited for local analysis (e.g. WGS84, NAD83 for United States, Lambert Equal Area for dynamically local studies, etc, even the butterfly projection). 

Keep in mind that the data we work with below are static variables that change on longer timescales (e.g. years). Annotating and obtaining spatio-temporally dynamic remote sensing products is outside of the scope of this tutorial. This includes things such as daily NDVI, temperature, precipitation, nightlights. Things like daily temperature and NDVI arel important and widely used variables for wildlife ecology in particular (e.g. niches don’t exist in 1D in the wild, and reviewers may ask for environmental variables when reviewing our papers which use a lot of anthropogenic variables).

For the rasters, we'll also work on creating buffers around our camera trap points to calculate averages . This is actually simpler with rasters than with shapefiles, as you have to have to decide whether to rasterize shapefiles before calculating a buffer, which introduces another layer of complexity. 

### Read in and visualize NDVI data 

The first raster we'll work with is another example of data you downloaded from the internet, but this time from the Schell lab Drive! This NDVI raster layer is at a 30m resolution and was downloaded using code from Google Earth Engine. It is a composite of summer season NDVI in 2020 in the Bay area, with cloud cover removed. Note that this data may not fit all NDVI uses, so exercise caution before using it for your research!

```{r}

# read in the raster -- might take a miinute 
ndvi_bay <- rast(here("data", "YH_NDVI2020_OACA-30-3857.TIF")) %>% 
 project("EPSG:32611") # put it in the right proj  -- note that this is a DIFFERENT function than we used for the sfs, from terra package 




```

## Additional (uncropped) raster layers: All NLCD, NLCD impervious surface 

Here we have a few other raster data sets that we won't focus on the smaller steps for because they're so large and we didn't upload cropped versions of them. However, it's still extremely useful to learn how to work with these large data sets and crop them yourself. In this section we'll make a small trick which is to crop our rasters to the area we are working on. 

### Read in NLCD data 
You can plot the NLCD first to see its all of the USA. This may be overkill and take up precious RAM resources on our machines (file is (~3.3 GB). If you have a slower or older machine, skip the plotting function.

```{r}

uwin_sf_buf = uwin_sf |> st_buffer(1000) 

# Landcover
CEC_map <- rast(
  "Data/NA_NALCMS_landcover_2020v2_30m.tif"
) 

# Create a data frame with bounding box coordinates
# Define bounding box coordinates in longitude and latitude (WGS84)
lon_min <- min(uwin$Long)
lon_max <- max(uwin$Long)
lat_min <- min(uwin$Lat)
lat_max <- max(uwin$Lat)

# Create a data frame with bounding box coordinates
bbox_df <- data.frame(
  lon = c(lon_min, lon_max, lon_max, lon_min, lon_min),
  lat = c(lat_min, lat_min, lat_max, lat_max, lat_min)
)


# Convert to an sf polygon
bbox_sf <- st_as_sf(bbox_df, coords = c("lon", "lat"), crs = 4326) %>%
  st_transform("EPSG:32611")
  summarise(geometry = st_combine(geometry)) %>%
  st_cast("POLYGON")

# Get the CRS of the raster
raster_crs <- crs(CEC_map)

# Transform the bounding box to match the raster's CRS
bbox_sf_proj <- st_transform(bbox_sf, crs = crs(CEC_map))

bbox_vect <- vect(bbox_sf_proj) # convert to spatvector
cropped_raster <- terra::crop(CEC_map, bbox_vect)

cropped_raster_r = raster(cropped_raster)

terra::plot(CEC_map)

plot(cropped_raster)

# Landcover type:
# Lets annotate the landcpver:
uwin_sf_landcov <- as(st_transform(st_as_sf(uwin_sf_buf),
                                   crs(CEC_map)),'Spatial')
uwin_sf_landcov_vect = vect(uwin_sf_landcov)

uwin_sf_landcov_vect$nldc_landcover = terra::extract(
  cropped_raster,
  uwin_sf_landcov_vect)[,2]

p_landcover = as_tibble(uwin_sf_landcov_vect) |>
  dplyr::select(Name, nldc_landcover)

# Now lets bring landcover back to our original data frame
all_sites <- all_sites %>% left_join(p_landcover, by = 'Name')
```


### Read in NLCD impervious surface data 
The above code uses the `raster` package, which is now deprecated and may not work in newer versions of R & RStudio. Here we'll extract buffers using a different method. This code will also print each buffer as it goes so you can make sure things look good. 
The link we provided is for the entire U.S. and is 24GB (!) so we'll make sure to crop it to our study area.

```{r}
imp_map <- rast(here("data", "NLCD_imp", 
                       "nlcd_2019_impervious_descriptor_l48_20210604.img")) %>% 
  terra::project("EPSG:32611")

# initialize new col for data
housing_dat <- st_drop_geometry(sites) %>%
  dplyr::mutate(POPDEN2020 = NA_real_)

# set buffer radius
buffer_radius <- 1000

Sys.time()

for (i in 1:nrow(all_sites)) {
  pt <- all_sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000,
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  housing_dat$POPDEN2020[i] <- extracted_value
  
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(rast_cropped, mapping = aes(x = x, y = y, fill = POPDEN2020)) +
    #   scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(2)
  print(i)
}

Sys.time()
```


### Other potential data sets 

Things we didn't cover in this tutorial include full NLCD data, elevation data, Human Footprint Index, and things like iNaturalist or OpenStreetMaps data. But now you have the skills to download and work with these datasets and we can touch on some of them next time! Note: to download elevation products you can simply Google SRTM download. This can come anywhere between 10m-90m depending on the extent of the product and the detail you may need. There is also airplane orthophoto imagery you could get for 1m2 elevation products!

## Creating buffers around data sets 
Next we'll learn how to create buffers around our camera points, which will allow us to actually conduct analyses with our spatial data. Here we'll read in the data a built in function (though you could read it in and then extract the data separately). We often don't really want to focus on a single camera location, but rather get conditions around a specific buffer for all of our sites. In this case we will create a 1000m buffer and get the mean value of several US census based metrics.

With shapefiles, you have two choices: use the shapefile to create a buffer by averaging the values of interest in the polygons and then creating the buffer, or rasterizing the polygon and then calculating the buffer based on the average of the entire circle. The method you will want to use depends on your project and hypotheses. We'll do both methods. 


### Housing data -- method 1
Here we have a handy function to calculate 1000m buffers around our camera points with multiple tidycensus variables. This function will give you the average population and housing density (another way to get housing density!) per square kilometer and print out a graph of the distribution of these variables. We'll try out different variables than the one we used above. For this function we'll read in the data frame version of the camera points.

We'll use the first method here:
```{r}

pop_housing_density = function(df, buffer_size){

  df_sf = st_as_sf(SpatialPointsDataFrame(df,
                                          coords = df[,c('Long', 'Lat')],
                                          proj4string =CRS("+proj=longlat +datum=WGS84")
  ))


# Download Income, Age, Population, and Housing Data
state_pop_age <- get_acs(
  state = 'CA',
  # county = c("Alameda", "Contra Costa"),
  geography = "block group",
  variables = c(
    pop = "B01003_001",       # Total Population
    housing = "B25001_001"    # Total Housing Units
  ),
  geometry = TRUE,
  year = 2020,
  output = "wide"
)

# Calculate Population and Housing Density
state_pop_age_density <- state_pop_age %>%
  mutate(
    area_sqkm = st_area(geometry) / 1e6,          # Convert area to square kilometers
    pop_density = as.numeric(popE / area_sqkm),              # Population density (people per sq km)
    housing_density = as.numeric(housingE / area_sqkm)       # Housing density (units per sq km)
  )

p_sf = df %>% st_as_sf(coords = c('Long', 'Lat'), crs = st_crs(4326))  %>% st_transform(st_crs(state_pop_age_density))
# Spatial join point locations and income
p_sf_pop_age_density = st_join(p_sf, state_pop_age_density)


if(
  is.na(buffer_size)
){

  col_pal = c('#046C9A', 'bisque3')

  ggplot_housing_density = ggplot() +
    geom_density(aes(housing_density,
                     fill = "Housing density across sites"),
                 alpha = .2,
                 data = p_sf_pop_age_density, linewidth = 0.8)  +
    geom_density(aes(housing_density, fill = "Background Housing density"), alpha = .2, data = state_pop_age_density, linewidth = 0.8) +
    ggtitle('UWIN \n across housing density of census tracts ') +
    scale_fill_manual(values = col_pal) + theme_classic() + ylab('Sampling density') + xlab('Housing density') +
    theme(axis.text.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.text.y = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.y = element_text(face = "bold", size = 16 ,color='black')) # +
  # theme(legend.position="none") # Remove legend

  print(ggplot_housing_density)


  return(p_sf_pop_age_density)
}


if(
  !is.na(buffer_size)
){
  df_sf_buffer <- p_sf %>%
    st_buffer(dist = buffer_size) # For example 1000m buffer

  buffered_point_joined_pop_age_density <- st_join(
    df_sf_buffer,
    state_pop_age_density,
    join = st_intersects,
    left = FALSE)


  # Calculate the Mean Median Household Income Within Each Buffer
  mean_pop_house_d_buff_p <- buffered_point_joined_pop_age_density %>%
    group_by(Name) %>%
    summarize(mean_housing_density = mean(housing_density, na.rm = TRUE),
              mean_pop_density = mean(pop_density, na.rm = TRUE))

  df_sf_w_mean_inc_age_buf <- df %>%
    left_join(mean_pop_house_d_buff_p, by = "Name") |>
    # Now to get it back to our original data framer annotate the mean income age and provide the size of the buffer
    dplyr::select(Name, mean_pop_density, mean_housing_density) |>
    mutate(buffer_size = paste0(buffer_size))

  col_pal = c('#046C9A', 'bisque3')

  ggplot_housing_density = ggplot() +
    geom_density(aes(mean_housing_density,
                     fill = "Housing density across UWIN"),
                 alpha = .2,
                 data = df_sf_w_mean_inc_age_buf, linewidth = 0.8, fill = '#046C9A')  +
    geom_density(aes(housing_density, fill = "Background Housing density"), alpha = .2, fill = 'bisque3', data = state_pop_age_density, linewidth = 0.8) +
    ggtitle(paste0('UWIN \n across housing density of census tracts , buffer size = ', buffer_size)) +
    scale_fill_manual(values = col_pal) + theme_classic() + ylab('Sampling density') + xlab('Housing density') +
    theme(axis.text.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.text.y = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.y = element_text(face = "bold", size = 16 ,color='black')) # +
  # theme(legend.position="none") # Remove legend

  print(ggplot_housing_density)

  return(df_sf_w_mean_inc_age_buf)
}

}


# here we'll input our points data frame into the function and specify the buffer size 
uwin_sf_sf_pop_hous_dens_1000m =
  pop_housing_density(all_sites_df, 1000) |> dplyr::select(-buffer_size)



```


### Income data -- method 2
Here we'll use method 2 (rasterizing the shapefile and writing a loop to calculate each buffer) using the income data we got from tidycensus -- this method also prints the result of each buffer in a plot: 
```{r}

# first let's rasterize the sf from ealier 

# need to create a template 
template <- rast(ext(tractsSF), resolution=100, crs="EPSG:32611") # be careful with resolution here! 

# now perform rasterize
rast <- terra::rasterize(vect(tractsSF), template, field = "estimate")

# initialize new col for data 
income_dat <- st_drop_geometry(all_sites) %>%
  dplyr::mutate(med_income = NA_real_)

# set buffer radius
buffer_radius <- 1000

Sys.time()

for (i in 1:nrow(all_sites)) {
  pt <- all_sites[i, ]  # iterate through sites 
  
  # create buffer
  buff <- vect(st_transform(pt, crs(rast)))
  buff_terra <- terra::buffer(buff, width = buffer_radius) 
  
  # need to give it a bit extra extent 
  buff_ext <- ext(buff_terra) 
  buff_ext <- ext(c(
    xmin = buff_ext$xmin - 1000, # this just gives the buffer a buffer so to speak 
    xmax = buff_ext$xmax + 1000,
    ymin = buff_ext$ymin - 1000,
    ymax = buff_ext$ymax + 1000))
  buff_terra <- terra::crop(buff_terra, buff_ext)
  
  # crop and mask the raster 
  rast_cropped <- terra::crop(rast, buff_terra)
  rast_masked <- terra::mask(rast_cropped, buff_terra)
  
  # xxtract values within the buffer, calculate mean
  extracted_value <- exact_extract(rast_masked, st_as_sf(buff_terra), fun = "mean", 
                                   weights = "area")
  income_dat$med_income[i] <- extracted_value
  
  # Plot the results
  
  p <- ggplot() +
    geom_raster(as.data.frame(rast_cropped, xy = TRUE), mapping = aes(x = x, y = y, fill = estimate)) +
    #    scale_fill_manual() +
    geom_sf(data = st_as_sf(buff_terra), fill = NA, color = "red", size = 1) +
    geom_sf(data = all_sites[i,], color = "blue", size = 3) +
    coord_sf(crs = st_crs(rast), datum = st_crs(rast)) +
    labs(title = paste("Site:", i),
         x = "Easting (m)", y = "Northing (m)",
         fill = "Raster Value") +
    theme_minimal()
  
  # Save the plot
  # ggsave(filename = paste0("plots/plot_site_", i, ".png"), plot = p, width = 8, height = 6)
  print(p) 
  Sys.sleep(1)
  print(i)
}

Sys.time()

# Check the final results
print(income_dat)

# we won't bind this to our data set for now but you'll want to make sure to do so in a real analysis

```


### CalEnviroScreen data 

We haevn't read in CalEnviroScreen yet, but now that we have the hang of the process we can speed it up a bit. We downloaded the CalEnviroScreen data from their website and uploaded it to the Schell Lab Drive. This is in .gdb format, but works similarly to shapefiles. We'll use method 1 here. 
```{r}

cenv <- st_read(here("data", "calenviroscreen40gdb_F_2021.gdb", "calenviroscreen40gdb_F_2021.gdb") %>% 
  dplyr::select(
    Tract, ZIP, Population, CIscore, CIscoreP, PM2_5, PM2_5_Pctl,
    Pesticides, Pesticides_Pctl, Tox_Releases, Tox_Releases_Pctl,
    Traffic, Traffic_Pctl,
    Solid_Waste, Solid_Waste_Pctl, Pollution, PollutionScore,
    Pollution_Pctl, Poverty, Poverty_Pctl,
    HousBurd, HousBurd_Pctl,
    ApproxLoc, Shape_Area, County) %>% 
    
    st_as_sf(coords = c('Long', 'Lat'), crs = "EPSG:4326"))  %>%
  st_transform(crs("EPSG:32611"))
  
# Inputs are a data frame, calenviroscreen, buffer size
get_cenv = function(points, cenv,  buffer_size=NA){
# cenv: Cal Enviro Screen GDB 
  # points is camera sf 
  # buffer_size in meters
# cenv_col_name: Cal Enviroscreen column name
  

   # If no buffer, return
   if(
     is.na(buffer_size)
   ){
     # Spatial join point locations and income
     p_sf_cenv = st_join(p_sf, cenv)
     return(p_sf_cenv)
   }
   
   # If buffer:
   if(
     !is.na(buffer_size)
   ){
     df_sf_buffer <- p_sf %>%
       st_buffer(dist = buffer_size) # For example 1000m buffer
     
     buffered_point_joined_cenv <- st_join(
       df_sf_buffer,
       cenv, 
       join = st_intersects,
       left = FALSE)
     
     # Calculate the Mean Cenv Variables scores Within Each Buffer
     # Using total scores, not percentiles
     mean_cenv_buff_p <- buffered_point_joined_cenv %>%
       group_by(Name) %>%
       summarize(mean_CIscore = mean(CIscore, na.rm = TRUE),
                 mean_PM2_5 = mean(PM2_5, na.rm = TRUE),
                 mean_Pesticides = mean(Pesticides, na.rm = TRUE),
                 mean_Tox_Releases = mean(Tox_Releases, na.rm = TRUE),
                 mean_Traffic = mean(Traffic, na.rm = TRUE),
                 mean_Solid_Waste = mean(Solid_Waste, na.rm = TRUE),
                 mean_Pollution = mean(Pollution, na.rm = TRUE),
                 mean_Poverty = mean(Poverty, na.rm = TRUE),
                 mean_HousBurd = mean(HousBurd, na.rm = TRUE)
                 
                 )
     
     df_sf_w_mean_cenv_buf <- df %>%
       left_join(mean_cenv_buff_p, by = "Name") |>
       # Now to get it back to our original data framer annotate the mean income age and provide the size of the buffer
       # dplyr::select(Name, mean_income, mean_age) |> 
       mutate(buffer_size = paste0(buffer_size))
     
     return(df_sf_w_mean_cenv_buf)
   }
   
  }

df_cenv = get_cenv(all_sites, cenv, 1000)

names(df_cenv)

df_cenv_v2 = df_cenv |> dplyr::select(Name, mean_Pesticides, mean_Tox_Releases, mean_Traffic,
                  mean_Solid_Waste, mean_Pollution, mean_Poverty, mean_HousBurd, buffer_size)


uwin_sf_v7 = uwin_sf_v6 |>
  left_join(df_cenv_v2)

```

### NDVI data
NDVI is a bit special because you have to decide how exactly you're going to classify the data. Below is code adapted from Travis Gallo that calculates a proportion of a buffer that is covered in vegetation classified as NDVI 0.6 or higher. Use caution before using this -- the values you'd want to use as a cutoff may differ markedly based on your study area, and you may not want to use a proportion at all! 

```{r}

# calculate the proportion of a site that is covered in vegetation (greater than 0.6)

# initialize a data frame to store results
prop_ndvi_greater0.6 <- data.frame(site = integer(), proportion_vegetation = numeric())

# loop through each site
for (i in 1:nrow(all_sites)) {
  # Create a buffer for the current site
  buffered_site <- terra::buffer(vect(all_sites[i, ]), width = 1000)  # Adjust buffer width as needed
  
  # Extract values from the NDVI raster using the buffered site
  buffered_values <- terra::extract(ndvi_bay, buffered_site, df = TRUE)
  
  # Calculate the proportion of NDVI values greater than 0.5
  if (nrow(buffered_values) > 0) {
    # Replace NA values with 0 for the calculation
    ndvi_values <- ifelse(is.na(buffered_values[, 2]), 0, buffered_values[, 2])
    
    # Calculate the proportion
    proportion_vegetation <- sum(ndvi_values > 0.6) / length(ndvi_values)
  } else {
    proportion_vegetation <- NA  # Handle case where no values are extracted
  }
  
# Store the results
  prop_ndvi_greater0.6 <- rbind(prop_ndvi_greater0.6, data.frame(site = all_sites[i, ]$Name, proportion_vegetation = proportion_vegetation))
}
```


That's it for part 1 -- next time we'll work on making pretty maps in ggplot! 