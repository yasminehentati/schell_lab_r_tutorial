---
title: "schell_lab_R_spatial_tutorial"
output: html_document
date: "2025-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Schell Lab R Spatial Tutorial
## Exploring remote sensing, census based and climatic datasets for urban ecology
### by Diego Ellis Soto and Yasmine Hentati
### February 25, 2025

This markdown assumes basic knowledge about GIS (e.g. rasters vs shapefiles, points vs lines, raster vs raster stack) and spatial ecology (e.g. issues related to grain size, spatial extent, spatio-temporal scale).

The motivation of this markdown is to provide a basic primer for downloading, annotating, visualizing and summarizing mostly static spatial data and relate it to UWIN camera trap locations in the Bay Area, California.

We'll work on both reading in and saving shapefiles/rasters in your study area, as well as extracting buffers from different layers around study area points (i.e. camera traps). 


## Packages
All of these are quite straightforward except for tidycensus, for which you'll need an API key. Use the directions under 2.1 [here](https://walker-data.com/census-r/an-introduction-to-tidycensus.html) to get it (just the short section before 2.1.1). Use the skeleton code below.

```{r}
install.packages("pacman") # this package just helps us quickly install and load a lot of packages at once 
pacman::p_load(tidycensus, sf, terra, mapview, tidyr, dplyr, readr, here, ggplot2)

# see above link to get your tidycensus key and paste it below 
census_api_key("YOUR KEY HERE",
               install = TRUE)

Sys.getenv("CENSUS_API_KEY") # this should print your API key back to you 
```

## Reading in points 

Let's start by loading our UWIN trap camera and visualizing it.

First we'll read in our .csv and turn it into a spatial format R can work with (sf). Here we'll work with the UWIN/Schell Lab East Bay camera trap points (saved as a .csv directly from the first tab of the Master UWIN Bay Area spreadsheet). I like to use WGS84 / UTM projection (easting/northing) instead of ones that use lat/long, so we'll also convert the points so we can project it in UTMs. This will be useful to know if someone provides you with data in a certain projection and you want to change it to another projection.

```{r}

# read in data 
all_sites <- read_csv(here("data", "eastbay_sites.csv")) %>% 
  slice (1:72) %>% # keep only rows with data
  select(1:17) %>% # keep only columns we might need 
  rename("Name" = "Site Names")


```

Let's convert this dataframe to a spatial SF object and plot it using mapview.
```{r}
# convert the data frame into an sf format using the points columns 
all_sites <- st_as_sf(all_sites, coords=c("Long", "Lat"), crs="EPSG:4326") #the EPSG string is simply a project code -- 4326 is the code for WGS84 lat/long, which google maps and other systems use 

# use mapview to look at our sites and make sure everything seems to be in the right place 
mapview(all_sites) 

# transform all_sites to UTM with st_transform 
all_sites <- all_sites %>% st_transform("EPSG:32611") # EPSG code for WGS84 UTM zone 11 (california)

# take a look at it again
mapview(all_sites)
st_crs(all_sites) # check out our projection system 
```

### Read in median income data using tidycensus 

Instead of working with a shapefile we downloaded from the Internet, we'll use a nifty R package to download the data for us. Tidycensus uses data from American Community Survey with many different years of data available. 

The US census has hundreds of interesting variables, but some other variables we could explore in the future are income, road networks, racial demographics.

The link in the first packages section above is a great overall tutorial if you want to learn more.

```{r}

# load in ACS 2019 variables 
v19 <- load_variables(2019, "acs5", cache = TRUE)

# now load in sf  of california census tracts 
# with median household income as variable of interest
tractincomeCA <- get_acs(state = "CA", 
                         geography = "tract", 
                         variables = c(medincome="B19013_001"), # this is just the code for the med income data, i think i found it in the linked tutorial
                         geometry = TRUE) %>% 
  st_transform(crs = "EPSG:32611") # correct projection

# take a look at the data set
glimpse(tractincomeCA)
class(tractincomeCA)

# now let's only keep census tracts we want so the file isn't huge -- let's do the entire SF bay area in case we need the rest of this data one day
tractsSF <- tractincomeCA %>% dplyr::filter(substr(GEOID, 1, 5) 
                                            # these are GEOIDs which represent each county ("06" represents the state of CA) -- these can be found online 
                                            %in% c("06055", "06041", # napa, marin
                                                   "06095", "06013", # solano, contra costa 
                                                   "06001", "06075", # alameda, SF 
                                                   "06081", "06085")) # san mateo, san clara 

# view as a map 
mapview(tractsSF)

# now let's write this to a shapefile saved in our directory 
st_write(tractsSF, here("data", "sf_bay_med_income.shp"),
         append = FALSE)

# next we'll extract 
```

### Read in housing density data 
This is an example of how to work with a shapefile you downloaded from the Internet. This data is publicly available from the Silvis lab. 
```{r}

bay_housing <- st_read(here("data", "CA_wui_block_1990_2020_change_v4.shp")) %>% 
  st_transform(crs = "EPSG:32611")# let's put it in the same projection we're working in 


# filter to only bay area counties
bay_housing <- ca_housing %>% dplyr::filter(substr(BLK20, 1, 5) # this is housing density by block for 2020 
                                           %in% c("06055", "06041", # same counties as before
                                                  "06095", "06013",
                                                  "06001", "06075",
                                                  "06081", "06085"))
```


### Rasters 
Awesome, we've learned how to read in and work with shapefiles! Now we can load some remote sensing variables in the form of rasters. These often come in the form of .tif or .img files. We can load these with the terra package. Older resources you find online might use the raster package, but it has since been deprecated. 

Common issues when working with rasters are: pixel size, projection, and extent. We also have to worry about projection and extent for shapefiles. It's especially important to keep track of these things when working in R, because you don't get as many opportunities to see what you are doing as in programs like ArcGIS. We will NOT cover how to mask water in this tutorial (e.g. apply an ocean mask), but you may also need to do this in study areas such as in the Bay.

Briefly: if you want to combine multiple rasters together in one ‘stack’ for analysis, these typically need to be at the same spatial extent and resolution. Since we are only extracting values of these rasters separately, we just need to ensure that these are in the same projection of our points (WGS84 in this example). There are literally hundreds of projections and many more are more suited for local analysis (e.g. WGS84, NAD83 for United States, Lambert Equal Area for dynamically local studies, etc, even the butterfly projection). 

Keep in mind that the data we work with below are static variables that change on longer timescales (e.g. years). Annotating and obtaining spatio-temporally dynamic remote sensing products is outside of the scope of this tutorial. This includes things such as daily NDVI, temperature, precipitation, nightlights. Things like daily temperature and NDVI arel important and widely used variables for wildlife ecology in particular (e.g. niches don’t exist in 1D in the wild, and reviewers may ask for environmental variables when reviewing our papers which use a lot of anthropogenic variables).

For the rasters, we'll also work on creating buffers around our camera trap points to calculate averages . This is actually simpler with rasters than with shapefiles, as you have to have to decide whether to rasterize shapefiles before calculating a buffer, which introduces another layer of complexity. 

### Read in NDVI data 

The first raster we'll work with is another example of data you downloaded from the internet, but this time from the Schell lab Drive! This NDVI raster layer is at a 30m resolution and was downloaded using code from Google Earth Engine. It is a composite of summer season NDVI in 2020 in the Bay area, with cloud cover removed. Note that this data may not fit all NDVI uses, so exercise caution before using it for your research! 

```{r}

# read in the raster 
ndvi_bay <- rast(here("data", "YH_NDVI2020_OACA-30-3857.TIF")) %>% 
 project("EPSG:32611") # put it in the right proj  -- note that this is a DIFFERENT function than we used for the sfs, from terra package 


```

## Additional raster layers: All NLCD, NLCD impervious surface 

Here we have a few other raster data sets that we won't focus on in the main tutorial because they're so large and we didn't upload cropped versions of them. However, it's still extremely useful to learn how to work with these large data setts. In this section we'll make a small trick which is to crop our rasters to the area we are working on. 


### Read in NLCD data 
Plot the NLCD first to see its all of the USA. This may be overkill and take up precious RAM resources on our machines (file is (~3.3 GB). If you have a slower or older machine, skip the plotting function.
```{r}

```


### Read in NLCD impervious surface data 
The link we provided is for the entire U.S. and is 24GB (!) so we'll make sure to crop it to our study area.

```{r}

```


### All NLCD data 

### Other potential data sets 

Things we didn't cover in this tutorial include elevation data, Human Footprint Index, and things like iNaturalist or OpenStreetMaps data. But now you have the skills to download and work with these datasets! Note: to download elevation products you can simply Google SRTM download. This can come anywhere between 10m-90m depending on the extent of the product and the detail you may need. There is also airplane orthophoto imagery you could get for 1m2 elevation products!

```{r}

```


## Creating buffers around data sets 
Next we'll learn how to create buffers around our camera points, which will allow us to actually conduct analyses with our spatial data. Here we'll read in the data a built in function (though you could read it in and then extract the data separately). We often don't really want to focus on a single camera location, but rather get conditions around a specific buffer for all of our sites. In this case we will create a 1000m buffer and get the mean value of several US census based metrics.


### Income data 
Here we have a handy function to calculate 1000m buffers around our camera points with multiple tidycensus variables. This function will give you the average population and housing density per square kilometer and print out a graph of the distribution of these variables. We'll try out different variables than the one we used above.
```{r}

pop_housing_density <- function(sf, buffer_size)
  {

# Download Population and Housing Data
state_pop_age <- get_acs(
  state = 'CA',
  geography = "block group",
  variables = c(
    pop = "B01003_001",       # Total Population
    housing = "B25001_001"    # Total Housing Units
  ),
  geometry = TRUE,
  year = 2020,
  output = "wide"
)

# Calculate Population and Housing Density -- square kilometers
state_pop_age_density <- state_pop_age %>%
  mutate(
    area_sqkm = st_area(geometry) / 1e6,          # Convert area to square kilometers
    pop_density = as.numeric(popE / area_sqkm),              # Population density (people per sq km)
    housing_density = as.numeric(housingE / area_sqkm)       # Housing density (units per sq km)
  )

p_sf = df %>% st_as_sf(coords = c('Long', 'Lat'), crs = st_crs(4326))  %>% st_transform(st_crs(state_pop_age_density))

# Spatial join point locations and income
p_sf_pop_age_density = st_join(p_sf, state_pop_age_density)


if(
  is.na(buffer_size)
){

  col_pal = c('#046C9A', 'bisque3')

  ggplot_housing_density = ggplot() +
    geom_density(aes(housing_density,
                     fill = "Housing density across sites"),
                 alpha = .2,
                 data = p_sf_pop_age_density, linewidth = 0.8)  +
    geom_density(aes(housing_density, fill = "Background Housing density"), alpha = .2, data = state_pop_age_density, linewidth = 0.8) +
    ggtitle('UWIN \n across housing density of census tracts ') +
    scale_fill_manual(values = col_pal) + theme_classic() + ylab('Sampling density') + xlab('Housing density') +
    theme(axis.text.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.text.y = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.y = element_text(face = "bold", size = 16 ,color='black')) # +
  # theme(legend.position="none") # Remove legend

  print(ggplot_housing_density)


  return(p_sf_pop_age_density)
}


if(
  !is.na(buffer_size)
){
  df_sf_buffer <- p_sf %>%
    st_buffer(dist = buffer_size) # For example 1000m buffer

  buffered_point_joined_pop_age_density <- st_join(
    df_sf_buffer,
    state_pop_age_density,
    join = st_intersects,
    left = FALSE)


  # Calculate the Mean Pop Density and Housing Density Within Each Buffer
  mean_pop_house_d_buff_p <- buffered_point_joined_pop_age_density %>%
    group_by(Name) %>%
    summarize(mean_housing_density = mean(housing_density, na.rm = TRUE),
              mean_pop_density = mean(pop_density, na.rm = TRUE))

  df_sf_w_mean_pop_house_buf <- df %>%
    left_join(mean_pop_house_d_buff_p, by = "Name") |>
    # Now to get it back to our original data framer annotate the mean income age and provide the size of the buffer
    dplyr::select("Name", mean_pop_density, mean_housing_density) |>
    mutate(buffer_size = paste0(buffer_size))

  col_pal = c('#046C9A', 'bisque3')

  ggplot_housing_density = ggplot() +
    geom_density(aes(mean_housing_density,
                     fill = "Housing density across UWIN"),
                 alpha = .2,
                 data = df_sf_w_mean_pop_house_buf, linewidth = 0.8, fill = '#046C9A')  +
    geom_density(aes(housing_density, fill = "Background Housing density"), alpha = .2, fill = 'bisque3', data = state_pop_age_density, linewidth = 0.8) +
    ggtitle(paste0('UWIN \n across housing density of census tracts , buffer size = ', buffer_size)) +
    scale_fill_manual(values = col_pal) + theme_classic() + ylab('Sampling density') + xlab('Housing density') +
    theme(axis.text.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.x = element_text(face = "bold", size = 16 ,color='black'),
          axis.text.y = element_text(face = "bold", size = 16 ,color='black'),
          axis.title.y = element_text(face = "bold", size = 16 ,color='black')) # +
  # theme(legend.position="none") # Remove legend

  print(ggplot_housing_density)

  return(df_sf_w_mean_pop_house_buf)
}

}

uwin_sf_sf_pop_hous_dens_1000m =
  pop_housing_density(all_sites, 1000) |> dplyr::select(-buffer_size)


# here we'll input our sf that we made above into the function and specify the buffer size 
```

### CalEnviroScreen data 

We haevn't read in CalEnviroScreen yet, but now that we have the hang of the process we can speed it up a bit. We downloaded the CalEnviroScreen data from their website and uploaded it to the Schell Lab Drive. This is in .gdb format, but works similarly to shapefiles. 
```{r}

cenv <- st_read(here("data", "calenviroscreen40gdb_F_2021.gdb", "calenviroscreen40gdb_F_2021.gdb") %>% 
  dplyr::select(
    Tract, ZIP, Population, CIscore, CIscoreP, PM2_5, PM2_5_Pctl,
    Pesticides, Pesticides_Pctl, Tox_Releases, Tox_Releases_Pctl,
    Traffic, Traffic_Pctl,
    Solid_Waste, Solid_Waste_Pctl, Pollution, PollutionScore,
    Pollution_Pctl, Poverty, Poverty_Pctl,
    HousBurd, HousBurd_Pctl,
    ApproxLoc, Shape_Area, County) %>% 
    
    st_as_sf(coords = c('Long', 'Lat'), crs = "EPSG:4326"))  %>%
  st_transform(st_crs("EPSG:32611"))
  
# Inputs are a data frame, calenviroscreen, buffer size
get_cenv = function(points, cenv,  buffer_size=NA){
# cenv: Cal Enviro Screen GDB 
  # points is camera sf 
  # buffer_size in meters
# cenv_col_name: Cal Enviroscreen column name
  

   # If no buffer, return
   if(
     is.na(buffer_size)
   ){
     # Spatial join point locations and income
     p_sf_cenv = st_join(p_sf, cenv)
     return(p_sf_cenv)
   }
   
   # If buffer:
   if(
     !is.na(buffer_size)
   ){
     df_sf_buffer <- p_sf %>%
       st_buffer(dist = buffer_size) # For example 1000m buffer
     
     buffered_point_joined_cenv <- st_join(
       df_sf_buffer,
       cenv, 
       join = st_intersects,
       left = FALSE)
     
     # Calculate the Mean Cenv Variables scores Within Each Buffer
     # Using total scores, not percentiles
     mean_cenv_buff_p <- buffered_point_joined_cenv %>%
       group_by(Name) %>%
       summarize(mean_CIscore = mean(CIscore, na.rm = TRUE),
                 mean_PM2_5 = mean(PM2_5, na.rm = TRUE),
                 mean_Pesticides = mean(Pesticides, na.rm = TRUE),
                 mean_Tox_Releases = mean(Tox_Releases, na.rm = TRUE),
                 mean_Traffic = mean(Traffic, na.rm = TRUE),
                 mean_Solid_Waste = mean(Solid_Waste, na.rm = TRUE),
                 mean_Pollution = mean(Pollution, na.rm = TRUE),
                 mean_Poverty = mean(Poverty, na.rm = TRUE),
                 mean_HousBurd = mean(HousBurd, na.rm = TRUE)
                 
                 )
     
     df_sf_w_mean_cenv_buf <- df %>%
       left_join(mean_cenv_buff_p, by = "Name") |>
       # Now to get it back to our original data framer annotate the mean income age and provide the size of the buffer
       # dplyr::select(Name, mean_income, mean_age) |> 
       mutate(buffer_size = paste0(buffer_size))
     
     return(df_sf_w_mean_cenv_buf)
   }
   
  }

df_cenv = get_cenv(uwin, cenv, 1000)

names(df_cenv)

df_cenv_v2 = df_cenv |> dplyr::select(Name, mean_Pesticides, mean_Tox_Releases, mean_Traffic,
                  mean_Solid_Waste, mean_Pollution, mean_Poverty, mean_HousBurd, buffer_size)


uwin_sf_v7 = uwin_sf_v6 |>
  left_join(df_cenv_v2)

```


That's it for part 1 -- next time we'll work on rasterizing shapefiles to extract buffer from them in a slightly different way, as well as making pretty maps in ggplot! 